import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

## @params: [JOB_NAME]
args = getResolvedOptions(sys.argv, ['JOB_NAME'])

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

mongodburl = mongodb+srv://admin:xxxxxxxxxxxxxx@plato-stage-2.qtjfk2s.mongodb.net/test
username = ""
password = ""
mongodburl = ""
source_databasename = ""
source_tablename

onprem_mongodb_url = "mongodb://" + username + ":" + password + "@" + mongodburl

target_databasename = ""
target_tablename = ""

onprem_mongodb_df = spark.read \
        .format("mongo") \
        .option("uri", onprem_mongodb_url) \
        .option("database", source_databasename) \
        .option("collection", source_tablename) \
#        .option("pipeline", pipeline) \
        .load()

rec_count = onprem_mongodb_df.count()
print("DocumentDB Record counts for Archival : ", rec_count)
onprem_mongodb_df.show()


onprem_mongodb = DynamicFrame.fromDF(df, glueContext,"dynamicdf")
write_documentdb_options = {
                        "uri": documentdburl,
                        "database": databasename,
                        "collection": documentdbtablename,
                        "username": username,
                        "password": password,
                        "ssl": "true",
                        "ssl.domain_match": "false"
                    }
glueContext.write_dynamic_frame.from_options(onprem_mongodb, connection_type="documentdb", connection_options=write_documentdb_options)

job.commit()